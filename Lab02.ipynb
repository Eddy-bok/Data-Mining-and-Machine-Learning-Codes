{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0d563b-1289-45a1-a7f0-fea6c1ca747c",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:3rem;color:skyblue;\">Advanced Lab: Python, NumPy, and Linear Regression Conceptsh1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34cf1493-fb0b-4277-8a46-27c35962838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34144722-2395-4f50-a45c-85024c31a029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = [1 2 3 4]\n",
      "b = [5 6 7 8]\n"
     ]
    }
   ],
   "source": [
    "# Create arrays\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([5, 6, 7, 8])\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0c8ef30-7a62-4bfd-9202-2bc59d81accc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b = [ 6  8 10 12]\n",
      "a * b = [ 5 12 21 32]\n",
      "np.dot(a,b) = 70\n"
     ]
    }
   ],
   "source": [
    "# Element-wise operations\n",
    "c = a + b\n",
    "d = a * b\n",
    "print(f\"a + b = {c}\")\n",
    "print(f\"a * b = {d}\")\n",
    "\n",
    "# Dot product\n",
    "e = np.dot(a, b)\n",
    "print(f\"np.dot(a,b) = {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60296ccd-7d72-444a-ba3e-fa8074f33f91",
   "metadata": {},
   "source": [
    "#### Vectorization VS Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "805c9f48-69fe-40f7-8fb0-873dfe7eaf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy time: 0.006994 seconds\n",
      "Loop time: 0.273730 seconds\n",
      "Speedup: 39.14x\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def loop_dot_product(a, b):\n",
    "    result = 0\n",
    "    for i in range(len(a)):\n",
    "        result += a[i] * b[i]\n",
    "    return result\n",
    "\n",
    "# Large vectors for performance comparison\n",
    "large_a = np.random.rand(1000000)\n",
    "large_b = np.random.rand(1000000)\n",
    "\n",
    "# Vectorized version\n",
    "start_time = time.time()\n",
    "np_result = np.dot(large_a, large_b)\n",
    "np_time = time.time() - start_time\n",
    "\n",
    "# Loop Version\n",
    "start_time = time.time()\n",
    "loop_result = loop_dot_product(large_a, large_b)\n",
    "loop_time = time.time() - start_time\n",
    "\n",
    "print(f\"NumPy time: {np_time:.6f} seconds\")\n",
    "print(f\"Loop time: {loop_time:.6f} seconds\")\n",
    "print(f\"Speedup: {loop_time / np_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef30fd3-3424-473c-a893-af7599662e32",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "##### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ada42714-8b22-4945-92e8-0bd5b50ab9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (6, 2)\n",
      "y shape: (6,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3], [3, 3], [3, 4]])\n",
    "y = np.array([250, 300, 320, 360, 400, 450])\n",
    "\n",
    "print(\"x shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883487ac-e5b3-468f-ade8-9c8d9511e9e4",
   "metadata": {},
   "source": [
    " #### Hypothesis and Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "75b1fa81-2cee-4156-863c-2c266e00fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(X, theta):\n",
    "    return np.dot(X, theta)\n",
    "\n",
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    predictions = hypothesis(X, theta)\n",
    "    cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58d89f3-dbc0-49b2-ae46-ad21ddf154ad",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d0feb7d-3c79-49ac-817f-eb9ecc3f4b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized theta:  [127.54510416  35.58721977  57.0452545 ]\n",
      "Final cost: 145.21369802516568\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    m = len(y)\n",
    "    j_history = []\n",
    "\n",
    "    for _ in range (num_iters):\n",
    "        predictions = hypothesis(X, theta)\n",
    "        theta = theta - (alpha / m) * np.dot(X.T, (predictions - y))\n",
    "        j_history.append(compute_cost(X, y, theta))\n",
    "    return theta, j_history\n",
    "\n",
    "# Add bias term to x\n",
    "X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "theta = np.zeros(X_b.shape[1])\n",
    "\n",
    "# Run gradient descent\n",
    "alpha = 0.01\n",
    "num_iters = 1000\n",
    "theta, j_history = gradient_descent(X_b, y, theta, alpha, num_iters)\n",
    "\n",
    "print(\"Optimized theta: \", theta)\n",
    "print(\"Final cost:\", j_history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c42c0a-8535-4ae7-982c-995c3e627394",
   "metadata": {},
   "source": [
    "#### Feature Scaling and Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4bccb2a3-8734-4376-9b28-9ccb52039c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z-score normalized X: \n",
      "[[-1.22474487 -1.5666989 ]\n",
      " [-1.22474487 -0.52223297]\n",
      " [ 0.         -0.52223297]\n",
      " [ 0.          0.52223297]\n",
      " [ 1.22474487  0.52223297]\n",
      " [ 1.22474487  1.5666989 ]]\n",
      "\n",
      "Min-Max normalized X:\n",
      "[[0.         0.        ]\n",
      " [0.         0.33333333]\n",
      " [0.5        0.33333333]\n",
      " [0.5        0.66666667]\n",
      " [1.         0.66666667]\n",
      " [1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "def normalize_z_score(X):\n",
    "    return (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "def normalize_min_max(X):\n",
    "    return (X - np.min(X, axis=0)) / (np.max(X, axis=0) - np.min(X, axis=0))\n",
    "\n",
    "# Apply Z-score normalize\n",
    "X_normalized_z = normalize_z_score(X)\n",
    "\n",
    "# Apply Min-Max normalization\n",
    "X_normalized_mm = normalize_min_max(X)\n",
    "\n",
    "print(\"Z-score normalized X: \")\n",
    "print(X_normalized_z)\n",
    "\n",
    "print(\"\\nMin-Max normalized X:\")\n",
    "print(X_normalized_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c456270e-bad5-41a1-8de5-396882797be7",
   "metadata": {},
   "source": [
    "##### Learning Rate Experimentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "30eff0b1-786d-4ea6-b8b4-27127788c186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning rate: 0.0001\n",
      "Final cost: 6238.036943740139\n",
      "Optimized theta: [20.44512837 42.08305931 52.6789439 ]\n",
      "\n",
      "Learning rate: 0.001\n",
      "Final cost: 1096.3935809601899\n",
      "Optimized theta: [42.3799654  55.23599469 71.4036938 ]\n",
      "\n",
      "Learning rate: 0.01\n",
      "Final cost: 145.21369802516568\n",
      "Optimized theta: [127.54510416  35.58721977  57.0452545 ]\n",
      "\n",
      "Learning rate: 0.1\n",
      "Final cost: 13.888889055967997\n",
      "Optimized theta: [173.33190135  28.33287061  46.66754512]\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "num_iters = 1000\n",
    "\n",
    "for alpha in learning_rates:\n",
    "    theta = np.zeros(X_b.shape[1])\n",
    "    theta, j_history = gradient_descent(X_b, y, theta, alpha, num_iters)\n",
    "\n",
    "    print(f\"\\nLearning rate: {alpha}\")\n",
    "    print(f\"Final cost: {j_history[-1]}\")\n",
    "    print(f\"Optimized theta: {theta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778fd368-66aa-42b3-ae9f-893458ff9ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
