{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "205298dd-c543-4dcb-abef-eaab8228c6ed",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:3rem;color:darkred;\">Predicting House Prices with Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efffdd1d-1bfb-48e8-acba-fc9a9b038f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9929fd3c-0e40-4c2a-a105-53a3ab72ca59",
   "metadata": {},
   "source": [
    "### Data Preparation and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5298a99d-2e57-4549-94c0-9c3c1d598024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features: [size (1000 sqft), number of bedrooms, age of house (years)]\n",
    "X = np.array([[1.0, 3, 10], [1.5, 3, 15], [2.0, 4, 5], [2.5, 4, 12], [3.0, 5, 8], [3.2, 5, 20]])\n",
    "y = np.array([300, 350, 480, 430, 630, 730]) # Prices in $1000s\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"y.shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c685b43-fd50-4971-b408-68770b18d9e2",
   "metadata": {},
   "source": [
    "### Implement the Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac76b73-6205-4bfe-a87c-44757316e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    m = X.shape[0] \n",
    "    total_cost = 0\n",
    "    for i in range(m):\n",
    "        f_wb = np.dot(X[i], w) + b\n",
    "        cost = (f_wb - y[i]) ** 2\n",
    "        total_cost += cost\n",
    "    total_cost = total_cost / (2 * m)\n",
    "    return total_cost\n",
    "\n",
    "# Test your function\n",
    "initial_w = np.zeros(X.shape[1])\n",
    "initial_b = 0\n",
    "cost = compute_cost(X, y, initial_w, initial_b)\n",
    "print(f\"Initial cost: {cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4348c9c-d009-4583-8722-370002f8d922",
   "metadata": {},
   "source": [
    "### Implement Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4dc07b-9a39-489e-8124-85e89139058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters):\n",
    "    m, n = X.shape\n",
    "    w = w_in.copy()\n",
    "    b = b_in\n",
    "    for i in range(num_iters):\n",
    "        dj_dw = np.zeros(n)\n",
    "        dj_db = 0\n",
    "        for j in range(m):\n",
    "            err = (np.dot(X[j], w) + b) - y[j]\n",
    "            for k in range(n):\n",
    "                dj_dw[k] += err * X[j][k]\n",
    "            dj_db += err\n",
    "        w = w - alpha * dj_dw / m\n",
    "        b = b - alpha * dj_db / m\n",
    "    return w, b\n",
    "\n",
    "# Run gradient descent:\n",
    "iterations = 1000\n",
    "alpha = 0.01\n",
    "w, b = gradient_descent(X, y, initial_w, initial_b, alpha, iterations)\n",
    "print(f\"Final w: {w}, Final b: {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03e055-b77c-4880-bb6d-66a4fdc4d051",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35f8503-7125-41bc-9152-5c58dbe72438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Z-score normalization\n",
    "def normalize_features(X):\n",
    "    return (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "X_normalized = normalize_features(X)\n",
    "\n",
    "# Re-run gradient descent:\n",
    "#iterations = 1000\n",
    "#alpha = 0.01\n",
    "w_norm, b_norm = gradient_descent(X_normalized, y, initial_w, initial_b, alpha, iterations)\n",
    "print(f\"Normalized: Final w: {w_norm}, Final b: {b_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e22ba7b-366e-4066-aa7d-c31597220243",
   "metadata": {},
   "source": [
    "###  Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d87a711-cb77-4b2f-9b13-641f4f59649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write prediction function\n",
    "def predict(X, w, b):\n",
    "    return np.dot(X, w) + b\n",
    "\n",
    "# Make predicitons for a new house\n",
    "new_house = np.array([2.8, 4, 18])  # 2800 sqft, 4 bedrooms, 18 years old\n",
    "new_house_normalized = (new_house - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "predicted_price = predict(new_house_normalized, w_norm, b_norm)\n",
    "print(f\"Predicted Price for the new house: ${predicted_price*1000:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc13b5a-2c08-47b9-9d09-c4ae89142c00",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4a4787-988f-4c90-aeba-88ca37f97112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimplement the cost function and gradient descent using vectorization:\n",
    "def compute_cost_vectorized(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "    f_wb = np.dot(X, w) + b\n",
    "    total_cost = np.sum((f_wb - y) ** 2) / (2 * m)\n",
    "    return total_cost\n",
    "\n",
    "def gradient_descent_vectorized(X, y, w_in, b_in, alpha, num_iters):\n",
    "    m, n = X.shape\n",
    "    w = w_in.copy()\n",
    "    b = b_in\n",
    "    for i in range(num_iters):\n",
    "        f_wb = np.dot(X, w) + b\n",
    "        dj_dw = np.dot(X.T, (f_wb - y)) / m\n",
    "        dj_db = np.sum(f_wb - y) / m\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "    return w, b\n",
    "\n",
    "# Compare execution time\n",
    "start_time = time.time()\n",
    "w_vect, b_vect = gradient_descent_vectorized(X_normalized, y, initial_w, initial_b, alpha, iterations)\n",
    "vect_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "w_non_vect, b_non_vect = gradient_descent(X_normalized, y, initial_w, initial_b, alpha, iterations)\n",
    "loop_time = time.time() - start_time\n",
    "\n",
    "print(f\"Vectorized time: {vect_time:.6f} seconds\")\n",
    "print(f\"Non-vectorized time: {loop_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896b6bd-6e30-4812-846d-b0709807f28c",
   "metadata": {},
   "source": [
    "### Analysis and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597e67b-d2eb-4633-b96d-2bb65721d89a",
   "metadata": {},
   "source": [
    "#### Cost Vs Iterations plot for Vectorized and non-vectorized implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d2423-2a28-4c43-bca6-693f3a66dcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_vectorized_cost(X, y, w_in, b_in, alpha, num_iters):\n",
    "    m, n = X.shape\n",
    "    w = w_in.copy()\n",
    "    b = b_in\n",
    "    cost_history = []\n",
    "    for i in range(num_iters):\n",
    "        f_wb = np.dot(X, w) + b\n",
    "        dj_dw = np.dot(X.T, (f_wb - y)) / m\n",
    "        dj_db = np.sum(f_wb - y) / m\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        cost_history.append(compute_cost(X, y, w, b))\n",
    "    return cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c495f71-466e-41f1-878a-1defcbb66cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_cost(X, y, w_in, b_in, alpha, num_iters):\n",
    "    m, n = X.shape\n",
    "    w = w_in.copy()\n",
    "    b = b_in\n",
    "    cost_hist = []\n",
    "    for i in range(num_iters):\n",
    "        dj_dw = np.zeros(n)\n",
    "        dj_db = 0\n",
    "        for j in range(m):\n",
    "            err = (np.dot(X[j], w) + b) - y[j]\n",
    "            for k in range(n):\n",
    "                dj_dw[k] += err * X[j][k]\n",
    "            dj_db += err\n",
    "        w = w - alpha * dj_dw / m\n",
    "        b = b - alpha * dj_db / m\n",
    "        cost_hist.append(compute_cost(X, y, w, b))\n",
    "    return cost_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7086b6c-422b-4a3f-8e9a-e5cc08ef9a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_vect = gradient_descent_vectorized_cost(X_normalized, y, initial_w, initial_b, alpha, iterations)\n",
    "cost_non_vect = gradient_descent_cost(X_normalized, y, initial_w, initial_b, alpha, iterations)\n",
    "\n",
    "plt.figure(figsize = (14, 6))\n",
    "# Vectorized Plot\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(iterations), cost_vect, label = 'Vectorized', linestyle = '--', color = 'darkblue', alpha=0.9)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Vs Iterations (Vectorized Implementation)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Non_Vectorized Plot\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(iterations), cost_non_vect, label = 'Non-Vectorized', linestyle = '-', color = 'red', alpha=0.4)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Vs Iterations (Non-Vectorized Implementation)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f967bf-6232-4415-ad16-75776ba1b3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6dadab1-295c-4012-b8db-ae5678e99081",
   "metadata": {},
   "source": [
    "#### Impact of Feature Scaling on Convergence of Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089db88-6668-417f-b14d-e6fb8830ce70",
   "metadata": {},
   "source": [
    "Feature scaling adjusts the features so they are on a similar scale.  In this code, we used the Z-score normalization technique.  This transformation centers the features around 0, with a standard deviation of 1.  Without feature scaling, the gradient descent algorithm convergence may take longer due to the imbalanced feature magnitude. Features like \"size(1.0 - 3.2)\" are much smaller than \"age of house(5 - 20)\" and would result in weights that are incomparable in magnitude.  Hence, it can be implied that feature scaling leads to faster convergence, improved numerical stability and makes scaled weights easier to interpret. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e9ef1-b61b-413b-8c3e-92df94cbbb63",
   "metadata": {},
   "source": [
    "#### Comparison of Efficiency of Vectorized and Non-Vectorized Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf6c7a-90cf-4c9b-96be-fad59c3d4f65",
   "metadata": {},
   "source": [
    "The implementation was on a small dataset (6 X 3), and the difference in execution time between the vectorized and non-vectorized implementations was negligible.  The result of the execution timing code above revealed the vectorized code execution at 0.012000 and the non-vectorized code at 0.015009 seconds. For a large dataset, the vectorized implementation will complete faster than the non-vectorized and this can be noted from the Lab(2), where we tested vectorization against non-vectorization with a large dataset. In comparison to the non-vectorized implementation, the vectorized implementation is more efficient for some reasons below;\n",
    "\n",
    "- It eliminates the reliance on python loop overheads:  Non-vectorized implementation relies heavily on python loops, which introduce this overhead for each iteration.\n",
    "- Quicker processing:  In vectorized implementation, operations are performed on entire matrices or vectors at once and not by looping, thereby reducing time and the overhead of loop iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce90687-cad2-42ed-adfa-22429c117c06",
   "metadata": {},
   "source": [
    "#### Some Limitations on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c65d0e-44fa-460c-b29a-dea90f6f3e4b",
   "metadata": {},
   "source": [
    "1. Assumption of linearity:  The model assumes a linear relationship between the features and target variable, and this might not necessarily be the case for all dataset.  If non-linear relationship is established, then the models accuracy will suffer.\n",
    "2. Limited Dataset:  The size of the dataset is too small and would limit the performance of the model. This limited size of the data could also lead to overfitting or underfitting in another instance.  The larger the dataset, the better the model performance will be.\n",
    "3. Limited features from the dataset.                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f90db52-4365-407d-8b93-09e74a1c9b66",
   "metadata": {},
   "source": [
    "#### Suggested Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15236d9a-3e56-45ad-a09b-d51a5229605a",
   "metadata": {},
   "source": [
    "Based on the above limitations some suggestions on model improvement include, but not limited to the following;\n",
    "1. Increase the dataset:  A larger dataset will improve the models ability to generalize and thereby reduce risk of overfitting or underfitting.  An increased dataset will improve the optimization process in the gradient descent and give the model more data to learn fro, thereby improving the overall accuracy of the model.\n",
    "2. Incorporate Additional Features:  The model currently uses only size, number of bedrooms, and age.  Additional features can significantly improve model accuracy.\n",
    "3. Feature Engineering:  We could consider introducing feature engineering to capture the more complex relationships between the existing features or even creating new ones from the original dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
